\documentclass{article}
\usepackage[bottom=2cm, right=1.5cm, left=1.5cm, top=2cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{exercise} % Exercises Style
\usepackage{graphicx}
\usepackage{caption}
\usepackage{environ}



% Enable Code
\usepackage{minted}
\let \extra T

\newcommand{\vect}[1]{\boldsymbol{#1}}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\E}{E}

\usepackage{fancyhdr}
\newenvironment{solution}
  {\renewcommand\qedsymbol{$\blacksquare$}\begin{proof}[Solution]$ $}
  {\end{proof}}

\title{Solutions to Assignment }
\author{Rongfei Jin}
\begin{document}

\pagestyle{fancy}
\fancyhf{}%
\fancyhead[L]{\textbf{ \ Assignment }}
\fancyhead[R]{\textbf{Rongfei Jin}}
\fancyfoot[C]{\thepage}%
\maketitle

\section*{Question 1}

We use the PACF to determine the order of the AR model. The PACF is defined as the correlation between $Y_t$ and $Y_{t-k}$ after removing the effect of all the intermediate variables $Y_{t-1}, Y_{t-2}, \ldots, Y_{t-(k-1)}$.

Consider the \(AR(p+1)\) model:
\begin{align*} 
Y_t &= a_0 + \sum_{k=1}^{p+1} a_k Y_{t-k} + \epsilon_t, \quad \epsilon_t
\end{align*}

Since the true model is \(AR(p)\), we have \(a_{p+1} = 0\) and the PACF should be zero for all lags greater than \(p\).

Now to determine the PACF, we can use the Yule-Walker equations, which relate the autocorrelations of the time series to the coefficients of the AR model and we can compute the significant levels to check if PACF is significantly different from zero.

See q1.py for the implementation.
\section*{Question 2}








\end{document}
% To estimate the $AR(p)$ model, we can use Maximal Likelihood Estimation (MLE). 
% First we notice that the conditional distribution of $Y_t$ given the past values is Gaussian since the only random variable in the model is $\epsilon$ itself. The model can be written as:

% \[Y_t | y_{t-1}, \ldots, y_{t-k} \sim N(a_0 + \sum_{k=1}^{p} a_k y_{t-k}, \sigma^2)\]

% then the conditional density function is:
% \begin{align*}
% f(Y_t = y_t | y_{t-1}, \ldots, y_{t-p}) &= \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_t - (a_0 + \sum_{k=1}^{p} a_k y_{t-k}))^2}{2\sigma^2}\right)\\
% \end{align*}

% The likelihood function for the entire sample is the product of the conditional densities:

% \begin{align*}
% L(a_0, a_1, \ldots, a_p, \sigma^2) &= \prod_{t=p+1}^{n} f(Y_t = y_t | y_{t-1}, \ldots, y_{t-p})\\
% &= \prod_{t=p+1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_t - (a_0 + \sum_{k=1}^{p} a_k y_{t-k}))^2}{2\sigma^2}\right)\\
% &= \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^{n-p} \exp\left(-\frac{1}{2\sigma^2} \sum_{t=p+1}^{n} (y_t - (a_0 + \sum_{k=1}^{p} a_k y_{t-k}))^2\right)
% \end{align*}

% To find the MLE, we take the logarithm of the likelihood function:
% \begin{align*}
% \log L(a_0, a_1, \ldots, a_p, \sigma^2) &= -\frac{n-p}{2} \log(2\pi) - \frac{(n-p)}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{t=p+1}^{n} (y_t - (a_0 + \sum_{k=1}^{p} a_k y_{t-k}))^2
% \end{align*}
